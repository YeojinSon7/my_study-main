# 4월 28일
# python
## 코딩연습
### 거꾸로 배열해도 같은 단어 또는 문장이 되는 회문인지 판별하는 함수를 정의하세요
### 함수이름: is_palindrome
### 반환값: True 또는 False
### 함수에 문자열을 입력받고 회문이면 True 아니면 False 반환하세요
```python
def is_palindrome(input_string):
    input_string = input_string.replace(" ","") # 문자열 공백 제거
    length = len(input_string)
    for i in range(length//2): # 계산할 때 몫 나오라고 // 쓴거고 회문은 반만가지고 전체 맨앞과 맨뒤 같은지 비교하면서 하면 된다
        if input_string[i] != input_string[length - 1 - i]:
            return False
    return True
    # return input_string == input_string[::-1] 위의 코드들 없이 이줄만 써줄수도 있다
print(is_palindrome("소주 만병만 주소"))
```

## reversed(값) 
- return이 뒤집어진 값이 나온다 리스트, 문자열등의 시퀀스형 쓸 수 있고 원본이 바뀌지 않는다
```python
string1 = "안녕하세요"
string2 = reversed(string1)
print(string2) # 안나옴
for i in string2:
    print(i) # 이렇게 해야 나온다 
```
## 참고
- 코딩테스트 (꾸준히 연습해라)
- 백준
- 소프트웨어 익스퍼트 아카데미 (삼성에서 운영하고있음)
- 프로그래머스 (카카오 연계 코테진행) (lv3,lv4 정도 되면 대기업입사할수있다?)

# 데이터 수집 단계 (데이터 크롤링)

- 머신러닝
- 데이터 수집 -> 데이터 분석/처리 -> 인공지능 모델 학습 -> 인공지능 모델 평가 -> 사용
- 데이터가 엄청엄청 많아야하고 좋아야지 인공지능이 잘된다
--------------------------
- http (hypertext transfer protocol) 프로토콜은 규약(약속)
- request - response (요청 - 응답) 
- client-server 서버가 준다 (사용자)(크롬같은 인터넷 브러우저가 클라이언트가된다)
- html (response 안에 까보면 있다)(hypertext markup language)
- 웹사이트를 표현하기 위한 언어
- <html></html>
- html 파싱 html을 잘 읽고 걸러낸다
----------------------------
## http 상태 코드
- 200 : OK
- 요청 성공
- 302 : 리다이렉트
- 다른 페이지로 바로 연결
- 400 : Bad Request 요청이 잘못됨
- 401 : Unauthorized 비인증됨
- 403 : Forbidden 접근 권한 없음
- 404 : Not Found 요청받은 내용이 없음
- 405: Method Not Allowed 접근 방법이 잘못됨
- 400번대는 client 리퀘스트 에러다
- 500: Internal Server Error 서버에러 # 개발자가 잘못한경우가 많다
- 501: Not Implemented
- 502: Bad Gatewqy 잘못된 응답
- 500번대는 서버 운영쪽 에러

## url구조
#- 프로토콜://호스트주소:포트번호/경로?쿼리
- http://naver.com/?~~~~
- 쿼리
- 쿼리이름 = 값&쿼리이름=값&쿼리이름=값

```python
search_url = "https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=" + "피자"
keyword = "머니"
url = search_url + keyword
response = requests.get(url)
response = requests.get(search_url)
print(response.text)
# %EC%B9%98%ED%82%A8 (치킨인데 이렇게 쓰여있다  왜냐면 깨져서 16진수로 표현하고있는것)
print(type(response.text))
```
## BeautifulSoup
- html 파싱(parsing)
- html을 객체 구조화해서 사용
----------------------------------  
- 구글이나 네이버등의 이미지를 가져와서 폴더에 저장하자
```python
import requests

url = "https://www.naver.com/"
response = requests.get(url)
status = response.status_code
html = response.text
print(status)
print(html)
from bs4 import BeautifulSoup
search_url = "https://www.google.co.kr/search?isch&q="
# html 태그
# <태그이름 속성=속성값>내용</태그이름>
keyword = "맥주"
url = search_url + keyword
import os
folder_name = "imgs"
if os.path.exists(folder_name):
    os.mkdir(folder_name)
img = imgs[1]
print(img)
print(img['src'])
for img in imgs[1 :]:
file_name = f"{keyword}_{idx}.jpg"
file_path = os.path.join(folder_name.file_name)
   img_response =  requests. get(img['src'])
   print(img['src']) # 이미지 주소 가져오기
    img_data= img_response.content
    print(img_data)
    with open(file_path,"wb") as f:
    f.write(img_data)
# 제대로 안된 코드다 다시 짜야함
```
```python
for idx, img in enumerate(imgs[1:]):
    print(idx,"번재 이미지 저장" )
html = "<html><body>Hello</body></html>"
soup = BeautifulSoup(html, "html.parser")
print(soup.body.text)
print(type(soup.body.text))

print(soup.find_all("div"))# 리스트형태로 가져온다 그래서 5번째 div태그 가져오려고 할 때 인덱스 쓸 수 있다

# html 클래스 python클래스와 다르다
imgs = soup.find_all('img', attrs ={'class' : ['_image_listImage']})
print(imgs)

container = soup.find('div',attrs={'id': 'container'})

print(container.find_all('img'))
# 제대로 안된 코드다 다시 짜야함
```

## enumerate(이터러블)
- 번호를 붙이는 함수
```python
li1 = ["김연아", "류현진", "손흥민"]
for idx, name in enumerate(li1):
    print(idx, name)
```

# 네이버 IT/과학 뉴스 크롤링

```python
import requests # 요청할 때
from bs4 import BeautifulSoup # html을 편하게 이용하기위해 객체형태로 파싱
url = "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=105"
response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}) # response가 결과로 온다 # 네이버 측에서 봇인줄알고 막아놔서 headers={"User-Agent": "Mozilla/5.0" 써줌 (크롤링 방지/회피)
html = response.text
soup = BeautifulSoup(html,"html.parser")
div = soup.body.find('div', attrs={'class':'list_body'})
print(div)
headlines= div.find_all('a',attrs={"class": "cluster_text_headline"})
for headline in headlines:
    print(headline.text.strip())
    with open("crawling_result/headlines.txt","a",encoding="utf-8") as f:
        f.write(headline.text.strip())
        f.write("\n")
    # crawling_result.headlines.txt =  print(headline.text.strip())

    article_response = requests.get(headline["href"])
    article_soup = BeautifulSoup(article_response.text,"html.parser")
    article= article_soup.find("div",attrs = {"id": "dic_area"})
    print(article.text)
#위의 코드는 제대로 작동한다
```
- a태그는 시진링크같은것은 포함된다
- a태그는 닻이라는 의미로 쓴다 그래서 링크로 해당되는 것 가져옴